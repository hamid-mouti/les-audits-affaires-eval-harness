name: ðŸ›ï¸ CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  release:
    types: [ published ]

env:
  PYTHON_VERSION: "3.9"

jobs:
  # Code Quality Checks
  quality:
    name: ðŸ” Code Quality
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Format check with Black
      run: black --check src/ tests/ scripts/
      
    - name: Import sorting check with isort
      run: isort --check-only src/ tests/ scripts/
      
    - name: Lint with flake8
      run: flake8 src/ tests/ scripts/
      
    - name: Type check with mypy
      run: mypy src/les_audits_affaires_eval/
      
    - name: Security check with bandit
      run: bandit -r src/les_audits_affaires_eval/
      
    - name: Dependency security check
      run: safety check

  # Unit Tests
  test:
    name: ðŸ§ª Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Run unit tests
      run: pytest tests/unit/ -v --cov=les_audits_affaires_eval --cov-report=xml
      
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Integration Tests (limited, no real API calls)
  integration-test:
    name: ðŸ”— Integration Tests
    runs-on: ubuntu-latest
    needs: [quality, test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Test CLI installation
      run: |
        lae-eval --help
        lae-eval info || true  # May fail without proper config, that's OK
        
    - name: Test package imports
      run: |
        python -c "from les_audits_affaires_eval import LesAuditsAffairesEvaluator; print('âœ… Main import works')"
        python -c "from les_audits_affaires_eval.clients import ModelClient; print('âœ… Client import works')"
        python -c "from les_audits_affaires_eval.utils import load_evaluation_results; print('âœ… Utils import works')"

  # Build Package
  build:
    name: ðŸ“¦ Build Package
    runs-on: ubuntu-latest
    needs: [quality, test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
        
    - name: Build package
      run: python -m build
      
    - name: Check package
      run: twine check dist/*
      
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist
        path: dist/

  # Documentation
  docs:
    name: ðŸ“š Documentation
    runs-on: ubuntu-latest
    needs: [quality]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Check README
      run: |
        python -c "import markdown; markdown.markdown(open('README.md').read())"
        echo "âœ… README.md is valid markdown"

  # Release to PyPI (only on GitHub releases, handled by auto-version.yml)
  release:
    name: ðŸš€ Release to PyPI
    runs-on: ubuntu-latest
    needs: [quality, test, integration-test, build, docs]
    if: github.event_name == 'release' && github.event.action == 'published'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
        
    - name: Build package
      run: python -m build
      
    - name: Publish to PyPI
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
      run: twine upload dist/*

  # Performance Benchmarks (optional, only on main branch)
  benchmark:
    name: âš¡ Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [test]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Run performance tests
      run: |
        echo "ðŸš€ Running performance benchmarks..."
        python -c "
        import time
        from les_audits_affaires_eval.config import get_safe_model_name
        
        # Simple performance test
        start = time.time()
        for i in range(1000):
            get_safe_model_name(f'test-model-{i}/version-1.0')
        end = time.time()
        
        print(f'âœ… Config function performance: {(end-start)*1000:.2f}ms for 1000 calls')
        "

  # Notification
  notify:
    name: ðŸ“¢ Notify
    runs-on: ubuntu-latest
    needs: [quality, test, integration-test, build, docs]
    if: always()
    
    steps:
    - name: Workflow Summary
      run: |
        echo "## ðŸ›ï¸ Les Audits-Affaires Evaluation Harness - CI Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Tests | ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration | ${{ needs.integration-test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Build | ${{ needs.build.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Documentation | ${{ needs.docs.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”— **Branch:** \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“ **Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ‘¤ **Author:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY 